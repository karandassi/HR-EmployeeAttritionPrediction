logit.fit.stats.compare.new1 <- rbind(logit.fit.stats.compare.new, logit.rates.25)
print(logit.fit.stats.compare.new1, digits=2)
print("Final Comparision of the four models:")
print(logit.fit.stats.compare.new1, digits=2)
# The first step with ROCR is to create a prediction(){ROCR} object using two vectors: (1) the predicted probabilities, computed above, and the actuas values. Notice that we use the index [test] to match the records in the predicted values
hra.logit.test
heart.fit = glm(chd ~ ., family=binomial(link="logit"), data=heart )
summary(heart.fit)
require(coefplot)
coefplot(heart.fit)
#Coefficient Plot:
coefplot(hra.logit)
#Coefficient Plot:
require(coefplot)
coefplot(hra.logit)
-2*logLik(hra.logit) # 2LL
deviance(hra.logit) # Should yield the same value
AIC(hra.logit) # 2LL + 2*Number of variables
-2*logLik(heart.fit) # 2LL
deviance(heart.fit) # Should yield the same value
AIC(heart.fit) # 2LL + 2*Number of variables
log.odds = coef(heart.fit) # To get just the coefficients
log.odds # Check it out
log.odds <- coef(hra.logit)
odds <- exp(coef(hra.logit))
prob <- odds/(1+odds)
cbind(log.odds, odds, prob)
confint(hra.logit)
#95 percent confidence intervals of odds.
exp(confint(hra.logit))
#95 percent confidence intervals of log odds.
confint(hra.logit)
hraReducedBusiness %>%
select(Attrition, Age, BusinessTravel, Department, DistanceFromHome, EnvironmentSatisfaction, Gender, JobSatisfaction, MaritalStatus, OverTime,   RelationshipSatisfaction, WorkLifeBalance, YearsSinceLastPromotion, YearsWithCurrManager) ->
hraReducedSignif
#Creating the reduced Logit model:
hra.logit.reduce <- glm(hraReducedSignif$Attrition ~ ., data=hraReducedSignif, family=binomial(link="logit"))
summary(hra.logit.reduce)
hraReducedBusiness %>%
select(Attrition, Age, BusinessTravel, Department, DistanceFromHome, EnvironmentSatisfaction, Gender, JobSatisfaction, MaritalStatus, OverTime,   RelationshipSatisfaction, WorkLifeBalance, YearsSinceLastPromotion, YearsWithCurrManager) ->
hraReducedSignif
#Creating the reduced Logit model:
hra.logit.reduce <- glm(hraReducedSignif$Attrition ~ ., data=hraReducedSignif, family=binomial(link="logit"))
summary(hra.logit.reduce)
#Coefficient Plot:
require(coefplot)
coefplot(hra.logit)
-2*logLik(hra.logit) # 2LL
deviance(hra.logit) # Should yield the same value
AIC(hra.logit) # 2LL + 2*Number of variables
coefplot(hra.logit.reduce)
#Coefficient Plot:
require(coefplot)
-2*logLik(hra.logit.reduce) # 2LL
deviance(hra.logit.reduce) # Should yield the same value
AIC(hra.logit.reduce) # 2LL + 2*Number of variables
#95 percent confidence intervals of log odds.
confint(hra.logit.reduce)
#95 percent confidence intervals of odds.
exp(confint(hra.logit.reduce))
#Check for accuracy using Confusion Matrix:
attach(hraReducedSignif)
set.seed(1)
train <- sample(1:nrow(hraReducedSignif), 0.7*nrow(hraReducedSignif))
test <- seq(1:nrow(hraReducedSignif))[-train]
hra.logit.train <- glm(Attrition ~ ., family=binomial(link="logit"), data=hraReducedSignif[train,])
summary(hra.logit.train)
hra.logit.test=predict(hra.logit.train, hraReducedSignif, type="response")[test]
hra.logit.test[1:10]
hra.pred.test = ifelse(hra.logit.test>0.5, 1,0)
hra.pred.test[1:10] # List first 10
conf.mat <- table("Predicted"=hra.pred.test, "Actual"=Attrition[test])
colnames(conf.mat) <- c("No", "Yes")
rownames(conf.mat) <- c("No", "Yes")
#Final Confusion Matrix
conf.mat
TruN <- conf.mat[1,1] # True negatives
TruP <- conf.mat[2,2] # True positives
FalN <- conf.mat[1,2] # False negatives
FalP <- conf.mat[2,1] # False positives
TotN <- conf.mat[1,1] + conf.mat[2,1] # Total negatives
TotP <- conf.mat[1,2] + conf.mat[2,2] # Total positives
Tot <- TotN+TotP # Total
Accuracy.Rate <- (TruN + TruP) / Tot
#It is quite clear that our accuracy overall is pretty good.
Accuracy.Rate
Error.Rate <- (FalN + FalP) / Tot
#The overall error rate is quite bad
Error.Rate
Sensitivity <- TruP / TotP
Sensitivity
Specificity <- TruN / TotN
Specificity
# False Positive Rate
FalseP.Rate <- 1 - Specificity
FalseP.Rate
logit.rates.50 <- c(Accuracy.Rate, Error.Rate, Sensitivity, Specificity, FalseP.Rate)
names(logit.rates.50) <- c("Accuracy Rate", "Error Rate", "Sensitivity", "Specificity", "False Positives")
print(logit.rates.50, digits=2)
pred <- prediction(hra.pred.test, Attrition[test])
perf <- performance(pred,"tpr","fpr")
plot(perf, colorize=T)
auc <- performance(pred,"auc")
c(auc@y.name[[1]], auc@y.values[[1]])
heart.probs.test=predict(heart.fit.train, heart, type="response")[test]
heart.probs.test[1:10] # List first 10
attach(heart) # Let's attach the dataset to simplify formulas
set.seed(1) # Set the random seed to get repeatable results
train <- sample(1:nrow(heart), 0.7*nrow(heart)) # train sub-sample (index vector)
test <- seq(1:nrow(heart))[-train] # test sub-sample (index vector)
heart.fit.train <- glm(chd ~ ., family=binomial(link="logit"), data=heart[train,])
summary(heart.fit.train)
heart.probs.test=predict(heart.fit.train, heart, type="response")[test]
heart.probs.test[1:10] # List first 10
heart.pred.test = ifelse(heart.probs.test>0.5, 1,0)
heart.pred.test[1:10] # List first 10
conf.mat <- table("Predicted"=heart.pred.test, "Actual"=chd[test])
colnames(conf.mat) <- c("No", "Yes")
rownames(conf.mat) <- c("No", "Yes")
conf.mat
TruN <- conf.mat[1,1] # True negatives
TruP <- conf.mat[2,2] # True positives
FalN <- conf.mat[1,2] # False negatives
FalP <- conf.mat[2,1] # False positives
TotN <- conf.mat[1,1] + conf.mat[2,1] # Total negatives
TotP <- conf.mat[1,2] + conf.mat[2,2] # Total positives
Tot <- TotN+TotP # Total
Accuracy.Rate <- (TruN + TruP) / Tot
Accuracy.Rate # Check it out
Error.Rate <- (FalN + FalP) / Tot
Error.Rate # Check it out
Sensitivity <- TruP / TotP # Proportion of correct positives
Sensitivity # Check it out
Specificity <- TruN / TotN # Proportion of correct negatives
Specificity
FalseP.Rate <- 1 - Specificity
FalseP.Rate
logit.rates.50 <- c(Accuracy.Rate, Error.Rate, Sensitivity, Specificity, FalseP.Rate)
names(logit.rates.50) <- c("Accuracy Rate", "Error Rate", "Sensitivity", "Specificity", "False Positives")
print(logit.rates.50, digits=2)
heart.pred.test.60 <- ifelse(heart.probs.test>0.6, 1,0)
heart.pred.test.60
# Cross tabulate Prediction with Actual
conf.mat <- table("Predicted"=heart.pred.test.60, "Actual"=chd[test])
conf.mat # Check it out
TruN <- conf.mat[1,1] # True negatives
TruP <- conf.mat[2,2] # True positives
FalP <- conf.mat[2,1] # False positives
FalN <- conf.mat[1,2] # False negatives
TotN <- conf.mat[1,1] + conf.mat[2,1] # Total negatives
TotP <- conf.mat[1,2] + conf.mat[2,2] # Total positives
Tot <- TotN+TotP # Total
Accuracy.Rate <- (TruN+TruP)/Tot
Accuracy.Rate # Check it out
Error.Rate <- (FalN+FalP)/Tot
Error.Rate # Check it out
Sensitivity <- TruP/TotP # Proportion of correct positives
Sensitivity # Check it out
Specificity <- TruN/TotN # Proportion of correct negatives
Specificity
FalP.Rate <- 1 - Specificity
FalP.Rate
logit.rates.60 <- c(Accuracy.Rate, Error.Rate, Sensitivity, Specificity, FalP.Rate)
names(logit.rates.60) <- c("Accuracy Rate", "Error Rate", "Sensitivity", "Specificity", "False Positives")
print(logit.rates.60, digits=2)
logit.fit.stats.compare <- rbind(logit.rates.50, logit.rates.60)
print(logit.fit.stats.compare, digits=2)
heart.probs.test
chd[test]
pred <- prediction(heart.probs.test, chd[test])
perf <- performance(pred,"tpr","fpr")
plot(perf, colorize=T)
auc <- performance(pred,"auc")
c(auc@y.name[[1]], auc@y.values[[1]])
pred <- prediction(hra.logit.test, Attrition[test])
pred <- prediction(hra.logit.test, Attrition[test])
#Some descriptive analysiis
summary(hraReducedBusiness)
#we can see that there is a lot of No's in Attrition meaning the proportion of staff attrition for our dataset is way higher.
#Finding the proportion of attrition employees
hraReducedBusiness %>%
group_by(Attrition) %>%
summarise (n = n()) %>%
mutate(freq = n / sum(n))
hra.logit <- glm(hraReducedBusiness$Attrition ~ ., data=hraReducedBusiness, family=binomial(link="logit"))
summary(hra.logit)
#Coefficient Plot:
require(coefplot)
coefplot(hra.logit)
#We can see that a lot of the coefficient's confidence intervals are away from zero and a lot are quite close to zero and significant.
# Fit statistics
-2*logLik(hra.logit) # 2LL
deviance(hra.logit) # Should yield the same value
AIC(hra.logit) # 2LL + 2*Number of variables
log.odds <- coef(hra.logit)
odds <- exp(coef(hra.logit))
prob <- odds/(1+odds)
cbind(log.odds, odds, prob)
#95 percent confidence intervals of log odds.
confint(hra.logit)
#95 percent confidence intervals of odds.
exp(confint(hra.logit))
attach(hraReducedBusiness)
set.seed(1)
train <- sample(1:nrow(hraReducedBusiness), 0.7*nrow(hraReducedBusiness))
test <- seq(1:nrow(hraReducedBusiness))[-train]
hra.logit.train <- glm(Attrition ~ ., family=binomial(link="logit"), data=hraReducedBusiness[train,])
summary(hra.logit.train)
# Predicted values using the fitted train model and the test data for HRA Attrition
hra.logit.test=predict(hra.logit.train, hraReducedBusiness, type="response")[test]
hra.logit.test[1:10]
#Convert proportions to actual 0's or 1's
hra.pred.test = ifelse(hra.logit.test>0.5, 1,0)
hra.pred.test[1:10] # List first 10
# Cross tabulate Prediction with Actual
conf.mat <- table("Predicted"=hra.pred.test, "Actual"=Attrition[test])
colnames(conf.mat) <- c("No", "Yes")
rownames(conf.mat) <- c("No", "Yes")
#Final Confusion Matrix
conf.mat
#We can see that our model does a very good job at predicting No's but is not that good at predicting Yes's
# Computing Fit Statistics
TruN <- conf.mat[1,1] # True negatives
TruP <- conf.mat[2,2] # True positives
FalN <- conf.mat[1,2] # False negatives
FalP <- conf.mat[2,1] # False positives
TotN <- conf.mat[1,1] + conf.mat[2,1] # Total negatives
TotP <- conf.mat[1,2] + conf.mat[2,2] # Total positives
Tot <- TotN+TotP # Total
# Now let's use these to compute accuracy and error rates
Accuracy.Rate <- (TruN + TruP) / Tot
#It is quite clear that our accuracy overall is pretty good.
Accuracy.Rate
Error.Rate <- (FalN + FalP) / Tot
#The overall error rate is quite bad
Error.Rate
# Sensitivity -- rate of correct positives
Sensitivity <- TruP / TotP
Sensitivity
#Prediction for true positives is quite bad
# Specificity -- rate of correct negatives
Specificity <- TruN / TotN
Specificity
#Predicting true negatives is really good with this model.
# False Positive Rate
FalseP.Rate <- 1 - Specificity
FalseP.Rate
logit.rates.50 <- c(Accuracy.Rate, Error.Rate, Sensitivity, Specificity, FalseP.Rate)
names(logit.rates.50) <- c("Accuracy Rate", "Error Rate", "Sensitivity", "Specificity", "False Positives")
print(logit.rates.50, digits=2)
#Convert proportions to actual 0's or 1's
hra.pred.test = ifelse(hra.logit.test>0.2, 1,0)
hra.pred.test[1:10] # List first 10
# Cross tabulate Prediction with Actual
conf.mat <- table("Predicted"=hra.pred.test, "Actual"=Attrition[test])
colnames(conf.mat) <- c("No", "Yes")
rownames(conf.mat) <- c("No", "Yes")
#Final Confusion Matrix
conf.mat
#We can see that our model does a very good job at predicting No's but is not that good at predicting Yes's
# Computing Fit Statistics
TruN <- conf.mat[1,1] # True negatives
TruP <- conf.mat[2,2] # True positives
FalN <- conf.mat[1,2] # False negatives
FalP <- conf.mat[2,1] # False positives
TotN <- conf.mat[1,1] + conf.mat[2,1] # Total negatives
TotP <- conf.mat[1,2] + conf.mat[2,2] # Total positives
Tot <- TotN+TotP # Total
# Now let's use these to compute accuracy and error rates
Accuracy.Rate <- (TruN + TruP) / Tot
Accuracy.Rate
Error.Rate <- (FalN + FalP) / Tot
#The overall error rate is quite bad
Error.Rate
# Sensitivity -- rate of correct positives
Sensitivity <- TruP / TotP
Sensitivity
# Specificity -- rate of correct negatives
Specificity <- TruN / TotN
Specificity
# False Positive Rate
FalseP.Rate <- 1 - Specificity
FalseP.Rate
logit.rates.20 <- c(Accuracy.Rate, Error.Rate, Sensitivity, Specificity, FalseP.Rate)
names(logit.rates.20) <- c("Accuracy Rate", "Error Rate", "Sensitivity", "Specificity", "False Positives")
print(logit.rates.20, digits=2)
logit.fit.stats.compare <- rbind(logit.rates.50, logit.rates.20)
print(logit.fit.stats.compare, digits=2)
#Again with lambda = 0.3
#Convert proportions to actual 0's or 1's
hra.pred.test = ifelse(hra.logit.test>0.3, 1,0)
hra.pred.test[1:10] # List first 10
# Cross tabulate Prediction with Actual
conf.mat <- table("Predicted"=hra.pred.test, "Actual"=Attrition[test])
colnames(conf.mat) <- c("No", "Yes")
rownames(conf.mat) <- c("No", "Yes")
#Final Confusion Matrix
conf.mat
#We can see that our model does a very good job at predicting No's but is not that good at predicting Yes's
# Computing Fit Statistics
TruN <- conf.mat[1,1] # True negatives
TruP <- conf.mat[2,2] # True positives
FalN <- conf.mat[1,2] # False negatives
FalP <- conf.mat[2,1] # False positives
TotN <- conf.mat[1,1] + conf.mat[2,1] # Total negatives
TotP <- conf.mat[1,2] + conf.mat[2,2] # Total positives
Tot <- TotN+TotP # Total
# Now let's use these to compute accuracy and error rates
Accuracy.Rate <- (TruN + TruP) / Tot
Accuracy.Rate
Error.Rate <- (FalN + FalP) / Tot
#The overall error rate is quite bad
Error.Rate
# Sensitivity -- rate of correct positives
Sensitivity <- TruP / TotP
Sensitivity
# Specificity -- rate of correct negatives
Specificity <- TruN / TotN
Specificity
# False Positive Rate
FalseP.Rate <- 1 - Specificity
FalseP.Rate
logit.rates.30 <- c(Accuracy.Rate, Error.Rate, Sensitivity, Specificity, FalseP.Rate)
names(logit.rates.30) <- c("Accuracy Rate", "Error Rate", "Sensitivity", "Specificity", "False Positives")
print(logit.rates.30, digits=2)
logit.fit.stats.compare.new <- rbind(logit.fit.stats.compare, logit.rates.30)
print(logit.fit.stats.compare.new, digits=2)
#Finally, with Lambda = 0.25
hra.pred.test = ifelse(hra.logit.test>0.25, 1,0)
hra.pred.test[1:10] # List first 10
# Cross tabulate Prediction with Actual
conf.mat <- table("Predicted"=hra.pred.test, "Actual"=Attrition[test])
colnames(conf.mat) <- c("No", "Yes")
rownames(conf.mat) <- c("No", "Yes")
#Final Confusion Matrix
conf.mat
#We can see that our model does a very good job at predicting No's but is not that good at predicting Yes's
# Computing Fit Statistics
TruN <- conf.mat[1,1] # True negatives
TruP <- conf.mat[2,2] # True positives
FalN <- conf.mat[1,2] # False negatives
FalP <- conf.mat[2,1] # False positives
TotN <- conf.mat[1,1] + conf.mat[2,1] # Total negatives
TotP <- conf.mat[1,2] + conf.mat[2,2] # Total positives
Tot <- TotN+TotP # Total
# Now let's use these to compute accuracy and error rates
Accuracy.Rate <- (TruN + TruP) / Tot
Accuracy.Rate
Error.Rate <- (FalN + FalP) / Tot
#The overall error rate is quite bad
Error.Rate
# Sensitivity -- rate of correct positives
Sensitivity <- TruP / TotP
Sensitivity
# Specificity -- rate of correct negatives
Specificity <- TruN / TotN
Specificity
# False Positive Rate
FalseP.Rate <- 1 - Specificity
FalseP.Rate
logit.rates.25 <- c(Accuracy.Rate, Error.Rate, Sensitivity, Specificity, FalseP.Rate)
names(logit.rates.25) <- c("Accuracy Rate", "Error Rate", "Sensitivity", "Specificity", "False Positives")
print(logit.rates.25, digits=2)
logit.fit.stats.compare.new1 <- rbind(logit.fit.stats.compare.new, logit.rates.25)
print("Final Comparision of the four models:")
print(logit.fit.stats.compare.new1, digits=2)
pred <- prediction(hra.logit.test, Attrition[test])
perf <- performance(pred,"tpr","fpr")
plot(perf, colorize=T)
auc <- performance(pred,"auc")
c(auc@y.name[[1]], auc@y.values[[1]])
hraReducedBusiness %>%
select(Attrition, Age, BusinessTravel, Department, DistanceFromHome, EnvironmentSatisfaction, Gender, JobSatisfaction, MaritalStatus, OverTime,   RelationshipSatisfaction, WorkLifeBalance, YearsSinceLastPromotion, YearsWithCurrManager) ->
hraReducedSignif
#Creating the reduced Logit model:
hra.logit.reduce <- glm(hraReducedSignif$Attrition ~ ., data=hraReducedSignif, family=binomial(link="logit"))
summary(hra.logit.reduce)
summary(hra.logit.reduce)
#Coefficient Plot:
require(coefplot)
coefplot(hra.logit.reduce)
coefplot(hra.logit.reduce)
-2*logLik(hra.logit.reduce) # 2LL
deviance(hra.logit.reduce) # Should yield the same value
AIC(hra.logit.reduce) # 2LL + 2*Number of variables
log.odds <- coef(hra.logit.reduce)
odds <- exp(coef(hra.logit.reduce))
prob <- odds/(1+odds)
cbind(log.odds, odds, prob)
#95 percent confidence intervals of log odds.
confint(hra.logit.reduce)
#95 percent confidence intervals of log odds.
confint(hra.logit.reduce)
#95 percent confidence intervals of odds.
exp(confint(hra.logit.reduce))
#95 percent confidence intervals of log odds.
confint(hra.logit.reduce)
#95 percent confidence intervals of odds.
exp(confint(hra.logit.reduce))
#Check for accuracy using Confusion Matrix:
attach(hraReducedSignif)
#Check for accuracy using Confusion Matrix:
attach(hraReducedSignif)
set.seed(1)
train <- sample(1:nrow(hraReducedSignif), 0.7*nrow(hraReducedSignif))
test <- seq(1:nrow(hraReducedSignif))[-train]
hra.logit.train <- glm(Attrition ~ ., family=binomial(link="logit"), data=hraReducedSignif[train,])
summary(hra.logit.train)
hra.logit.test=predict(hra.logit.train, hraReducedSignif, type="response")[test]
hra.logit.test[1:10]
hra.pred.test = ifelse(hra.logit.test>0.5, 1,0)
hra.pred.test[1:10] # List first 10
conf.mat <- table("Predicted"=hra.pred.test, "Actual"=Attrition[test])
colnames(conf.mat) <- c("No", "Yes")
rownames(conf.mat) <- c("No", "Yes")
#Final Confusion Matrix
conf.mat
TruN <- conf.mat[1,1] # True negatives
TruP <- conf.mat[2,2] # True positives
FalN <- conf.mat[1,2] # False negatives
FalP <- conf.mat[2,1] # False positives
TotN <- conf.mat[1,1] + conf.mat[2,1] # Total negatives
TotP <- conf.mat[1,2] + conf.mat[2,2] # Total positives
Tot <- TotN+TotP # Total
Accuracy.Rate <- (TruN + TruP) / Tot
#It is quite clear that our accuracy overall is pretty good.
Accuracy.Rate
Error.Rate <- (FalN + FalP) / Tot
#The overall error rate is quite bad
Error.Rate
Sensitivity <- TruP / TotP
Sensitivity
Specificity <- TruN / TotN
Specificity
# False Positive Rate
FalseP.Rate <- 1 - Specificity
FalseP.Rate
logit.rates.50 <- c(Accuracy.Rate, Error.Rate, Sensitivity, Specificity, FalseP.Rate)
names(logit.rates.50) <- c("Accuracy Rate", "Error Rate", "Sensitivity", "Specificity", "False Positives")
print(logit.rates.50, digits=2)
hra.pred.test = ifelse(hra.logit.test>0.2, 1,0)
hra.pred.test[1:10] # List first 10
conf.mat <- table("Predicted"=hra.pred.test, "Actual"=Attrition[test])
colnames(conf.mat) <- c("No", "Yes")
rownames(conf.mat) <- c("No", "Yes")
#Final Confusion Matrix
conf.mat
TruN <- conf.mat[1,1] # True negatives
TruP <- conf.mat[2,2] # True positives
FalN <- conf.mat[1,2] # False negatives
FalP <- conf.mat[2,1] # False positives
TotN <- conf.mat[1,1] + conf.mat[2,1] # Total negatives
TotP <- conf.mat[1,2] + conf.mat[2,2] # Total positives
Tot <- TotN+TotP # Total
Accuracy.Rate <- (TruN + TruP) / Tot
Accuracy.Rate
Error.Rate <- (FalN + FalP) / Tot
#The overall error rate is quite bad
Error.Rate
Sensitivity <- TruP / TotP
Sensitivity
Specificity <- TruN / TotN
Specificity
# False Positive Rate
FalseP.Rate <- 1 - Specificity
FalseP.Rate
logit.rates.20 <- c(Accuracy.Rate, Error.Rate, Sensitivity, Specificity, FalseP.Rate)
names(logit.rates.20) <- c("Accuracy Rate", "Error Rate", "Sensitivity", "Specificity", "False Positives")
print(logit.rates.20, digits=2)
logit.fit.stats.compare <- rbind(logit.rates.50, logit.rates.20)
print(logit.fit.stats.compare, digits=2)
hra.pred.test = ifelse(hra.logit.test>0.25, 1,0)
hra.pred.test[1:10] # List first 10
conf.mat <- table("Predicted"=hra.pred.test, "Actual"=Attrition[test])
colnames(conf.mat) <- c("No", "Yes")
rownames(conf.mat) <- c("No", "Yes")
#Final Confusion Matrix
conf.mat
TruN <- conf.mat[1,1] # True negatives
TruP <- conf.mat[2,2] # True positives
FalN <- conf.mat[1,2] # False negatives
FalP <- conf.mat[2,1] # False positives
TotN <- conf.mat[1,1] + conf.mat[2,1] # Total negatives
TotP <- conf.mat[1,2] + conf.mat[2,2] # Total positives
Tot <- TotN+TotP # Total
Accuracy.Rate <- (TruN + TruP) / Tot
Accuracy.Rate
Error.Rate <- (FalN + FalP) / Tot
#The overall error rate is quite bad
Error.Rate
Sensitivity <- TruP / TotP
Sensitivity
Specificity <- TruN / TotN
Specificity
# False Positive Rate
FalseP.Rate <- 1 - Specificity
FalseP.Rate
logit.rates.25 <- c(Accuracy.Rate, Error.Rate, Sensitivity, Specificity, FalseP.Rate)
names(logit.rates.25) <- c("Accuracy Rate", "Error Rate", "Sensitivity", "Specificity", "False Positives")
print(logit.rates.25, digits=2)
logit.fit.stats.compare <- rbind(logit.fit.stats.compare, logit.rates.25)
print(logit.fit.stats.compare, digits=2)
pred <- prediction(hra.logit.test, Attrition[test])
perf <- performance(pred,"tpr","fpr")
plot(perf, colorize=T)
auc <- performance(pred,"auc")
c(auc@y.name[[1]], auc@y.values[[1]])
install.packages(c("HH", "leaps", "perturb"))
suppressPackageStartupMessages(library(perturb))
collin.diag = colldiag(mod=hra.logit, scale=FALSE, center=FALSE, add.intercept=TRUE)
summary(hra.logit)
cor(hraReducedBusiness)
